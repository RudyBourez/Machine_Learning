{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Machine learning </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Définitions</font>\n",
    "\n",
    "Le machine learning permet de donner la capacité à une machine d'apprendre sans la programmer explicitement.  \n",
    "Cela consiste à développer un modèle mathématique par rapport à des données. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Différents types d'apprentissage</font>\n",
    "\n",
    "Il existe 3 types d'apprentissages :\n",
    "* Apprentissage supervisé\n",
    "* Apprentissage non-supervisé\n",
    "* Apprentissage par renforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Apprentissage supervisé</font>\n",
    "\n",
    "La machine reçoit des données caractérisées par des variables x (**features**) et annotées d'une variable y (**label** | **target**).  \n",
    "La machine doit apprendre à prédire y en fonction des features x.  \n",
    "Etapes à réaliser :\n",
    "* Constitution d'un **dataset** avec de nombreuses données\n",
    "* Choix du **modèle** de machine learning avec les **hyperparamètres**\n",
    "* Algorithme d'optimisation pour trouver les meilleurs paramètres : **phase d'entraînement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Modéle|Scaling |\n",
    "|---|---|\n",
    "|Régression Linéaire|non|  \n",
    "|Régression Logistique|non|\n",
    "|Régression linéaire (Lasso ou Ridge)|oui|\n",
    "|Random Forest, XGBoost, Arbre de décision|oui|\n",
    "|SVM|oui|\n",
    "|Naive Bayes (gaussian)|oui (Normalisation)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Scaling</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On scale les valeurs continues (valeurs discrétes => Encoder)\n",
    "* **<font color=\"red\">MinMaxScaler**</font> => 0 à 1 (Ex: [0,16,32] => [0,0.5,1])\n",
    "* **<font color=\"red\">Robust Scaler**</font> => prise en compte de l'espace inter-quartile (enlever les outliers)\n",
    "* **<font color=\"red\">Standard Scaler**</font> => moyenne à 0 et écart-type à 1 (Gaussienne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Modèles</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **<font color=\"red\">Régression linéaire**</font> :\n",
    "    * Utilisation d'une fonction affine pour prédire des valeurs\n",
    "    * Plusieurs méthodes de régularisation (Ridge, Lasso)\n",
    "* **<font color=\"red\">Régression Logistique**</font> : \n",
    "    * Extension de la régression linéaire appliquée à la classification\n",
    "    * Utilisation de la fonction sigmoîde pour transformer en probabilitès\n",
    "    * Nécessite une bonne séparabilité linéaire des classes\n",
    "    * Forte facilité d'explication\n",
    "* **<font color=\"red\">Arbre de décision**</font> : \n",
    "    * Etude de la target pour deux variables explicatives (arbre)\n",
    "    * Séparation (branches) linéaires (horizontales ou verticales)\n",
    "* **<font color=\"red\">Random Forest**</font>:\n",
    "    * Parallélisation des arbres\n",
    "    * Bagging de plusieurs arbres de décision\n",
    "    * Score final\n",
    "* **<font color=\"red\">XGBoost**</font> :\n",
    "    * Même principe que random Forest\n",
    "    * Séquentiel\n",
    "    * Boosting : chaque résultat d'arbre engendre une pondération pour les prochains arbres\n",
    "* **<font color=\"red\">SVM**</font> :\n",
    "    * Pareil qu'arbre de décision mais on trace un **hyperplan** (n-1 dimensions pour n features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Voting :  Faire plusieurs modèles puis prendre la valeur qui apparait majoritairement\n",
    "* Stacking : Créer un nouveau modèles prenant les résultats de plusieurs modèles comme features\n",
    "* Bagging : Moyennes ou majorité des résultats de plusieurs modèles homogénes (random forest => bagging d'arbre de décision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Apprentissage non supervisé</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Réduction de dimension ACP</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principe de **l'ACP**:\n",
    "* Réduire le nombre de features (overfitting)\n",
    "* Donner une visualisation graphique à notre jeu de données avec les nouvelles dimensions\n",
    "* Ressemble à la régression linéaire mais en utilisant des vecteurs ou des plans \n",
    "* Minimisation de la somme des normes de chaque projection sur le vecteur ou plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple :  transformation d'un plan en 3D à un plant en 2D  \n",
    "\n",
    "On cherchera à obtenir un nuage de points pour lequel les points sont correctement séparés (garder une bonne variabilité)  \n",
    "On quantifie cette variabilité à plusieurs dimensions avec **<font color=\"red\">l'inertie</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas d'un plan, on cherche d'abord à trouver un vecteur qui maximise la variabilité puis on trouve un deuxième vecteur orthogonale au premier qui maximise à son tout la variabilité. On peut ainsi de suite chercher n vecteurs orthogonaux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche par la suite à estimer le % d'inertie en fonction du nombre de dimensions puis on se fixe un seuil.  \n",
    "Ex: On désire avoir un % d'inertie de 95%, on va donc faire la somme cumulée des % d'inertie par dimension ajouté jusqu'à obtenir au moins 95%, ce qui va nous donner le nombre de dimension N minimale à garder aprés PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualité de représentation et contribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Qualité de représentation:   \n",
    "      Calcul du cos² pour chaque dimension ou chaque individu, plus la somme des cos² tend vers 1, au mieux la variable ou l'individu est interprétée \n",
    "* Contribution:  \n",
    "      corrélation² / somme(corrélation²)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
